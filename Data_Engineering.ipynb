{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "656px",
        "left": "1550px",
        "right": "20px",
        "top": "120px",
        "width": "322px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Data_Engineering.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gtxdatathon/GTX-Geothermal-Datathon-Marcelo/blob/main/Data_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zduapz7ghtn5"
      },
      "source": [
        "# GTX Bootcamp - Data Engineering \n",
        "<small>by [Marcelo Guarido](https://www.linkedin.com/in/mguarido/)</small>\n",
        "<br>\n",
        "\n",
        "In this notebook, the focus will be in the data preparation prior to modeling.\n",
        "\n",
        "## Summary\n",
        "\n",
        "* Data preparation\n",
        "    * Cleaning\n",
        "    * Imputation (with mean/median)\n",
        "    * Feature engineering\n",
        "\n",
        "## Load useful packages\n",
        "\n",
        "For the well log data analyses, the main packages used are:\n",
        "* Pandas\n",
        "* Matplotlib\n",
        "* Seaborn\n",
        "\n",
        "Also, from the file \"utils.py\", load the log visualization function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh6J1HNEhtn8"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.colors as colors\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (20.0, 10.0)\n",
        "inline_rc = dict(mpl.rcParams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9z3ZLGxhtn8"
      },
      "source": [
        "# If seaborn package is missing, uncomment and run the following line:\n",
        "# ! pip install seaborn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NPCZEoChtn8"
      },
      "source": [
        "## Taking a look at the data\n",
        "\n",
        "This data is from the [2016 ML contest](https://github.com/seg/2016-ml-contest), with the focus for *Facies Classification*.\n",
        "\n",
        "The provided data is a CSV file with well logs information from different wells. There are 5 well logs and 2 indicators:\n",
        "\n",
        "* Gamma ray (GR)\n",
        "* Resistivity (ILD\\_log10)\n",
        "* Photoelectric effect (PE)\n",
        "* Neutron-density porosity difference (DeltaPHI)\n",
        "* Average neutron-density porosity (PHIND)\n",
        "* Nonmarine/marine indicator (NM\\_M)\n",
        "* Relative position (RELPOS)\n",
        "\n",
        "The goal is to train a model able to classify 9 different types of rocks, as listed in the following table:\n",
        "\n",
        "| Facies | Description | Label | Adjacent Facies|\n",
        "| :---:  |    :---:    | :---: |      :---:     |\n",
        "|   1    | Nonmarine Sandstone | SS | 2 |\n",
        "|   2    | Nonmarine coarse siltstone | CSiS | 1,3 |\n",
        "|   3    | Nonmarine fine siltstone | FSiS | 2 |\n",
        "|   4    | Marine siltstone and shale | SiSh | 5 |\n",
        "|   5    | Mudstone | MS | 4,6 |\n",
        "|   6    | Wackestone | WS | 5,7,8 |\n",
        "|   7    | Dolomite | D | 6,8 |\n",
        "|   8    | Packstone-grainstone | PS | 6,7,9 |\n",
        "|   9    | Phylloid-algal bafflestone | BS | 7,8 |\n",
        "\n",
        "So, let's take a look at the data!\n",
        "\n",
        "Loading the CSV file to *pandas* dataframe and cheking some of the features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWsiAnmuhtn9"
      },
      "source": [
        "data = pd.read_csv('data/facies_vectors.csv')\n",
        "data.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj76mu9Jhtn9"
      },
      "source": [
        "print(sorted(data['Well Name'].unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9GR09D2htn9"
      },
      "source": [
        "print(sorted(data['Facies'].unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z2Mpa7Ehtn9"
      },
      "source": [
        "print(sorted(data['Formation'].unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL6zm16zhtn9"
      },
      "source": [
        "## Features treatment\n",
        "\n",
        "Before plotting the data, let's create the list of features names, colors, and check for missing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvXlrgaghtn9"
      },
      "source": [
        "features = list(data)[4:]\n",
        "facies_names = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS', 'WS', 'D', 'PS', 'BS']\n",
        "facies_colors = ['#F4D03F', '#F5B041','#DC7633','#6E2C00', '#1B4F72','#2E86C1', '#AED6F1', '#A569BD', '#196F3D']\n",
        "target = 'Facies'\n",
        "print(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7Xkvbfhhtn9"
      },
      "source": [
        "Let's check for missing data in all the data frame columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNhjGT5Yhtn9"
      },
      "source": [
        "print(data.isna().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baClsDmBhtn-"
      },
      "source": [
        "Wells that contain missing data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtXc6VP3htn-"
      },
      "source": [
        "wells_missing = list(data[data[\"PE\"].isna()][\"Well Name\"].unique())\n",
        "print(wells_missing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjA560g0htn-"
      },
      "source": [
        "# Features per well\n",
        "mpl.rcParams['figure.figsize'] = (30.0, 15.0)\n",
        "inline_rc = dict(mpl.rcParams)\n",
        "\n",
        "for w_idx, w in enumerate(np.unique(data[\"Well Name\"])):\n",
        "    ax = plt.subplot(3, 4, w_idx+1)\n",
        "    hist = np.logical_not(np.any(np.isnan(data[data[\"Well Name\"] == w][features].values), axis=0))\n",
        "    plt.bar(np.arange(len(hist)), hist, color=facies_colors, align='center')\n",
        "    ax.set_xticks(np.arange(len(hist)))\n",
        "    ax.set_xticklabels(features)\n",
        "    ax.set_yticks([0, 1])\n",
        "    ax.set_yticklabels(['miss', 'hit'])\n",
        "    ax.set_title(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca9JYRwOhtn-"
      },
      "source": [
        "## Missing values treatment - Mean fill\n",
        "\n",
        "Initially, the missing values will be filled simply by the median of the feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y15rXU_Bhtn-"
      },
      "source": [
        "data_fill_mean = data.fillna(data.mean())\n",
        "print(data_fill_mean.isna().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuGCBfb7htn-"
      },
      "source": [
        "## Plotting the logs\n",
        "\n",
        "Let's use our function *make_facies_log_plot* to visualize the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Pdk6oLihtn-"
      },
      "source": [
        "def make_facies_log_plot(logs, facies_colors):\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    #make sure logs are sorted by depth\n",
        "    logs = logs.sort_values(by='Depth')\n",
        "    cmap_facies = colors.ListedColormap(\n",
        "            facies_colors[0:len(facies_colors)], 'indexed')\n",
        "    \n",
        "    ztop=logs.Depth.min(); zbot=logs.Depth.max()\n",
        "    \n",
        "    cluster=np.repeat(np.expand_dims(logs['Facies'].values,1), 100, 1)\n",
        "    \n",
        "    f, ax = plt.subplots(nrows=1, ncols=6, figsize=(8, 12))\n",
        "    ax[0].plot(logs.GR, logs.Depth, '-g')\n",
        "    ax[1].plot(logs.ILD_log10, logs.Depth, '-')\n",
        "    ax[2].plot(logs.DeltaPHI, logs.Depth, '-', color='0.5')\n",
        "    ax[3].plot(logs.PHIND, logs.Depth, '-', color='r')\n",
        "    ax[4].plot(logs.PE, logs.Depth, '-', color='black')\n",
        "    im=ax[5].imshow(cluster, interpolation='none', aspect='auto',\n",
        "                    cmap=cmap_facies,vmin=1,vmax=9)\n",
        "    \n",
        "    divider = make_axes_locatable(ax[5])\n",
        "    cax = divider.append_axes(\"right\", size=\"20%\", pad=0.05)\n",
        "    cbar=plt.colorbar(im, cax=cax)\n",
        "    cbar.set_label((17*' ').join([' SS ', 'CSiS', 'FSiS', \n",
        "                                'SiSh', ' MS ', ' WS ', ' D  ', \n",
        "                                ' PS ', ' BS ']))\n",
        "    cbar.set_ticks(range(0,1)); cbar.set_ticklabels('')\n",
        "    \n",
        "    for i in range(len(ax)-1):\n",
        "        ax[i].set_ylim(ztop,zbot)\n",
        "        ax[i].invert_yaxis()\n",
        "        ax[i].grid()\n",
        "        ax[i].locator_params(axis='x', nbins=3)\n",
        "    \n",
        "    ax[0].set_xlabel(\"GR\")\n",
        "    ax[0].set_xlim(logs.GR.min(),logs.GR.max())\n",
        "    ax[1].set_xlabel(\"ILD_log10\")\n",
        "    ax[1].set_xlim(logs.ILD_log10.min(),logs.ILD_log10.max())\n",
        "    ax[2].set_xlabel(\"DeltaPHI\")\n",
        "    ax[2].set_xlim(logs.DeltaPHI.min(),logs.DeltaPHI.max())\n",
        "    ax[3].set_xlabel(\"PHIND\")\n",
        "    ax[3].set_xlim(logs.PHIND.min(),logs.PHIND.max())\n",
        "    ax[4].set_xlabel(\"PE\")\n",
        "    ax[4].set_xlim(logs.PE.min(),logs.PE.max())\n",
        "    ax[5].set_xlabel('Facies')\n",
        "    \n",
        "    ax[1].set_yticklabels([]); ax[2].set_yticklabels([]); ax[3].set_yticklabels([])\n",
        "    ax[4].set_yticklabels([]); ax[5].set_yticklabels([])\n",
        "    ax[5].set_xticklabels([])\n",
        "    f.suptitle('Well: %s'%logs.iloc[0]['Well Name'], fontsize=14,y=0.94)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcIbyfTihtn_"
      },
      "source": [
        "mpl.rcParams['figure.figsize'] = (8.0, 8.0)\n",
        "inline_rc = dict(mpl.rcParams)\n",
        "\n",
        "for wellName in wells_missing:\n",
        "    plt.figure(figsize=(3,4))\n",
        "    make_facies_log_plot(data_fill_mean[data_fill_mean['Well Name'] == wellName], facies_colors = facies_colors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KERGgP2Fhtn_"
      },
      "source": [
        "## Missing values treatment - Median fill\n",
        "\n",
        "Initially, the missing values will be filled simply by the median of the feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpfBVs-Whtn_"
      },
      "source": [
        "data_fill_median = data.fillna(data.median())\n",
        "print(data_fill_median.isna().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kUbrvO9htn_"
      },
      "source": [
        "## Plotting the logs\n",
        "\n",
        "Let's use our function *make_facies_log_plot* to visualize the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECbS_UtZhtn_"
      },
      "source": [
        "for wellName in wells_missing:\n",
        "    plt.figure(figsize=(3,4))\n",
        "    make_facies_log_plot(data_fill_median[data_fill_median['Well Name'] == wellName], facies_colors = facies_colors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13AFJZZUhtn_"
      },
      "source": [
        "# data = data_fill_mean\n",
        "data = data_fill_median"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgKBVyBKhtn_"
      },
      "source": [
        "## Plotting all wells"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yoxr6aJ7htn_"
      },
      "source": [
        "for wellName in data[\"Well Name\"].unique():\n",
        "    plt.figure(figsize=(3,4))\n",
        "    make_facies_log_plot(data[data['Well Name'] == wellName], facies_colors = facies_colors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktwe7j87htn_"
      },
      "source": [
        "## Removing \"broken\" well (data cleaning)\n",
        "\n",
        "The well *Recruit F9* is, apparently, not so... well!!\n",
        "\n",
        "Remove it from the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPX_Oeywhtn_"
      },
      "source": [
        "make_facies_log_plot(data[data['Well Name'] == 'Recruit F9'], facies_colors = facies_colors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFase4jPhtoA"
      },
      "source": [
        "data = data[data['Well Name'] != 'Recruit F9']\n",
        "print(data['Well Name'].unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQHqK1PchtoA"
      },
      "source": [
        "## Missing values treatment - is mean/median fill good?\n",
        "\n",
        "Missing data for *PE* are full in 2 wells:\n",
        "\n",
        "+ Alexander D\n",
        "+ Kimzey A\n",
        "\n",
        "Is a median fill a good strategy to complete the missing data?\n",
        "\n",
        "Let's check the wells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpT-0WefhtoA"
      },
      "source": [
        "make_facies_log_plot(data[data['Well Name'] == 'ALEXANDER D'], facies_colors = facies_colors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIv1SMddhtoA"
      },
      "source": [
        "make_facies_log_plot(data[data['Well Name'] == 'KIMZEY A'], facies_colors = facies_colors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AhAbwhIhtoA"
      },
      "source": [
        "# Regression as Data Imputation\n",
        "\n",
        "The *photoelectric effect* (PE) completely missing for two wells. Filling it with the median of all PE values is not an optimized solution, and can lead to a biased model.\n",
        "\n",
        "A more suitable solution would be to train a regression model using the other logs as input and the PE as target.\n",
        "\n",
        "<img src=\"figures/imputation.png\" width=\"1000\" align=\"center\">\n",
        "\n",
        "### **We are going to see how to use machine learning regression models for data imputation on the Bootcamp - Machine Learning on May 6th**\n",
        "\n",
        "However, as *homework*, you can try to do it by yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPk-tv6PhtoA"
      },
      "source": [
        "# Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w4il5JMhtoA"
      },
      "source": [
        "well_names = sorted(data['Well Name'].unique())\n",
        "for i in well_names:\n",
        "    print(i, sorted(data[data['Well Name'] == i].Facies.unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg8kwPYKhtoA"
      },
      "source": [
        "As during the regression block, define the features, target, facies colors, and facies names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IROnoaJFhtoA"
      },
      "source": [
        "features = list(data)[4:]\n",
        "facies_names = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS', 'WS', 'D', 'PS', 'BS']\n",
        "facies_names_dict = {1 : 'SS', 2 : 'CSiS', 3 : 'FSiS', 4 : 'SiSh', 5 : 'MS', 6 : 'WS', 7 : 'D', 8 : 'PS', 9 : 'BS'}\n",
        "facies_colors = ['#F4D03F', '#F5B041','#DC7633','#6E2C00', '#1B4F72','#2E86C1', '#AED6F1', '#A569BD', '#196F3D']\n",
        "target = 'Facies'\n",
        "print(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jyQvJMuhtoA"
      },
      "source": [
        "ax = sns.pairplot(data[features + [target]], \n",
        "                  hue = target, \n",
        "                  palette = facies_colors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lyab6dzPhtoA"
      },
      "source": [
        "## Feature augmentation\n",
        "\n",
        "From the pairs plot, some of the logs combination showed a circular pattern over the facies classification. Machine learning classifier models like to use linear classification boundaries between the classes. An approximate circular boundary would require a very complex model, and a large dataset to be trained.\n",
        "\n",
        "One way to avoid it is by *feature engineering*, where the original features are used to create new features that may help the predictions. Here, from the circular pattern observation, convert the features to an equivalent in polar coordinates sounds as a smart solution:\n",
        "\n",
        "<img src=\"figures/polcor.png\" width=\"800\">\n",
        "\n",
        "Having two features $X_1$ and $X_2$, the radius $r$ and angle $\\phi$ are computed by the following equations:\n",
        "\n",
        "$$r = \\sqrt{{X_1}^2 + {X_2}^2}$$\n",
        "$$\\phi = \\arctan\\left(\\frac{X_2}{X_1}\\right)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tR-P7Y-htoB"
      },
      "source": [
        "# Transforming the pair plots into polar coordinates\n",
        "def card2polwells(data_in, features_wells):\n",
        "    data_polar = data_in\n",
        "    fea_red = features_wells\n",
        "    name_temp = features_wells\n",
        "    for fea1 in features_wells:\n",
        "        del fea_red[0]\n",
        "        for fea2 in fea_red:\n",
        "            x = (data_in[fea1] - np.mean(data_in[fea1]))/max(data_in[fea1])\n",
        "            y = (data_in[fea2] - np.mean(data_in[fea2]))/max(data_in[fea2])\n",
        "            data_polar[fea1 + '_' + fea2 + '_rho'] = np.sqrt(x**2 + y**2)\n",
        "            data_polar[fea1 + '_' + fea2 + '_phi'] = np.arctan2(y, x)\n",
        "    \n",
        "    return(data_polar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Biv4tKh0htoB"
      },
      "source": [
        "features_in = ['GR','ILD_log10','DeltaPHI','PHIND','PE']\n",
        "data_polar = card2polwells(data, features_in)\n",
        "data_polar.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u1dj-qEhtoB"
      },
      "source": [
        "Other augmentations can be done, such as the gradient of the features, the square of the features, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbheNYiThtoB"
      },
      "source": [
        "## Gradient of the features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F590F_shtoB"
      },
      "source": [
        "def comp_grad(data, features, wnames, depth = 'Depth', wncol = 'Well Name'):\n",
        "    for feature in features:\n",
        "        data[feature + '_grad'] = 0\n",
        "    \n",
        "    for wname in wnames:\n",
        "        dataT = data[data[wncol] == wname]\n",
        "        \n",
        "        for feature in features:\n",
        "            dataF = dataT[feature].values\n",
        "            dz = abs(dataT[depth].values[1] - dataT[depth].values[0])\n",
        "            dataGrad = np.diff(dataF)/dz\n",
        "            dataGrad = np.append([0], dataGrad)\n",
        "            data.loc[data[data[wncol] == wname].index, feature + '_grad'] = dataGrad\n",
        "            \n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l3kPr7ZhtoB"
      },
      "source": [
        "data_aug = comp_grad(data = data_polar, features = features, wnames = data[\"Well Name\"].unique())\n",
        "data_aug.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIw7Ka6dhtoB"
      },
      "source": [
        "There are more data augmentation you can create, such as statiscal windows and clusters, but that is homework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzIkaFJghtoB"
      },
      "source": [
        "# $$\\text{Thank you!}$$"
      ]
    }
  ]
}